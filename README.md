# ü§ñ Chatbot de IA (MVP)

Este proyecto implementa un chatbot conversacional impulsado por un Large Language Model (LLM). Construido con una arquitectura de microservicios contenerizada con Docker Compose, el sistema busca ser modular, escalable y eficiente.

## ‚ú® Caracter√≠sticas Actuales

* **Asistente Conversacional:** Capaz de comprender el lenguaje natural y generar respuestas coherentes utilizando el modelo Llama 3.
* **Arquitectura de Microservicios:** Desacoplamiento de servicios (Frontend, Backend, API de Chatbot, Servidor LLM) para facilitar el desarrollo y escalabilidad.
* **Contenerizaci√≥n con Docker Compose:** Todos los componentes se ejecutan en contenedores Docker, simplificando el despliegue y asegurando la portabilidad.
* **Frontend Simplificado:** Una interfaz de usuario web b√°sica en HTML/CSS/JavaScript que permite la interacci√≥n con el chatbot.
* **Proxy Inverso Nginx:** Gestiona el tr√°fico del frontend y lo redirige al backend API.
* **Logging Integrado:** Captura informaci√≥n de logs en cada servicio para facilitar la depuraci√≥n y monitoreo.

## üöÄ Arquitectura del Proyecto (MVP)

El proyecto sigue una arquitectura de microservicios para la modularidad y escalabilidad.

_Diagrama de la arquitectura del chatbot, mostrando los componentes y el flujo de comunicaci√≥n._

1.  **`frontend` (Nginx + HTML/CSS/JS):**
    * **Nginx:** Sirve los archivos est√°ticos de la interfaz de usuario y act√∫a como proxy inverso para enrutar las solicitudes API al `backend`.
    * **HTML/CSS/JS:** Proporciona una interfaz de chat b√°sica y funcional para la interacci√≥n del usuario.
2.  **`backend` (Spring Boot - Java):**
    * Act√∫a como la capa de orquestaci√≥n intermedia.
    * Recibe las solicitudes del `frontend` (v√≠a Nginx) y las reenv√≠a al `chatbot-api`.
    * Punto ideal para implementar l√≥gica de negocio adicional, autenticaci√≥n y futuras integraciones.
3.  **`chatbot-api` (FastAPI - Python):**
    * El servicio principal para la interacci√≥n con el LLM.
    * Recibe la pregunta del usuario del `backend`.
    * Gestiona la comunicaci√≥n con el servidor Ollama.
    * Actualmente utiliza un prompt de sistema b√°sico y se conecta de forma s√≠ncrona a Ollama.
4.  **`ollama` (Servidor LLM Local):**
    * Contenedor Docker que aloja y sirve el modelo de lenguaje grande (LLM), `Llama 3`.
    * Permite ejecutar el LLM localmente sin depender de APIs externas, ofreciendo mayor control.

## üõ†Ô∏è Tecnolog√≠as Utilizadas

* **Frontend:** Nginx, HTML, CSS, JavaScript (puro)
* **Backend:** Java (Spring Boot), Maven
* **API del Chatbot:** Python (FastAPI), `requests`, `pydantic`, `logging`
* **Modelo de Lenguaje:** Llama 3 (servido v√≠a Ollama)
* **Contenerizaci√≥n:** Docker, Docker Compose

## ‚öôÔ∏è C√≥mo Ejecutar el Proyecto

Aseg√∫rate de tener Docker y Docker Compose instalados en tu sistema.

1.  **Clona el Repositorio:**

2.  **Iniciar Ollama y Descargar el Modelo:**
    * Primero, levanta solo el servicio de Ollama para asegurarte de que est√© listo para descargar el modelo.
    ```bash
    docker compose build ollama
    docker compose up -d
    docker exec -it ollama ollama run llama3
    
3.  **Verificar que todo funcione:**
        ‚ó¶ Ollama: http://localhost:11434 
        ‚ó¶ API Python: http://localhost:8000 
        ‚ó¶ Backend Java: http://localhost:8080 
        ‚ó¶ Frontend: http://localhost:3000 

    
    
    
    
